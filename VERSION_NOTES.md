<!-- created at: 2026-02-05 12:30:00 -->
# 版本说明 - 基于Unix域套接字的RDMA资源隔离系统 v1.0

## 当前版本架构概述

本版本采用基于Unix域套接字的进程间通信架构，实现了RDMA资源的实时监控与动态拦截。

## 核心组件架构

```
┌─────────────────┐    ┌──────────────────┐    ┌──────────────────┐
│   Application │───▶│ librdma_intercept│───▶│ collector_server │
│                 │    │     (Intercept)  │    │   (Control)      │
└─────────────────┘    └──────────────────┘    └──────────────────┘
                              │                          │
                              ▼                          ▼
                       ┌──────────────────┐    ┌──────────────────┐
                       │   eBPF Program │◀───▶│   eBPF Maps      │
                       │    (Monitor)     │    │ (Resource Data)  │
                       └──────────────────┘    └──────────────────┘
```

## 关键技术特点

### 1. Unix域套接字通信
- **通信路径**：`/tmp/rdma_collector.sock`
- **通信协议**：自定义二进制协议，支持GET_STATS和GET_PROC_STATS命令
- **优势**：本地通信，无需网络协议栈，性能优于TCP/IP套接字
- **劣势**：相比共享内存仍有数据拷贝开销

### 2. eBPF监控机制
- **监控方式**：使用kprobe动态跟踪内核RDMA函数
- **监控函数**：
  - `ib_uverbs_create_qp`：QP创建跟踪
  - `ib_uverbs_destroy_qp`：QP销毁跟踪
  - `ib_uverbs_reg_mr`：内存注册跟踪
  - `ib_uverbs_dereg_mr`：内存注销跟踪
- **数据存储**：使用eBPF maps存储全局和进程级资源使用统计

### 3. 动态拦截策略
- **拦截方式**：LD_PRELOAD技术注入用户程序
- **拦截函数**：
  - `ibv_create_qp`：QP创建拦截
  - `ibv_destroy_qp`：QP销毁拦截
  - `ibv_reg_mr`：内存注册拦截
  - `ibv_dereg_mr`：内存注销拦截
- **决策逻辑**：基于eBPF数据和配置限制进行实时决策

## 性能特征

### 吞吐量测试
- **基准性能**：5357-7249 QPs/second（无拦截）
- **拦截性能**：4485-5599 QPs/second（有拦截）
- **性能开销**：16-22%

### 延迟测试
- **基准延迟**：0.507 ms/操作（无拦截）
- **拦截延迟**：0.640 ms/操作（有拦截）
- **延迟开销**：26%

## 使用流程

1. **启动eBPF监控**：
   ```bash
   sudo ./rdma_monitor
   ```

2. **启动collector_server**：
   ```bash
   ./collector_server
   ```

3. **运行用户程序**：
   ```bash
   LD_PRELOAD=./librdma_intercept.so RDMA_INTERCEPT_ENABLE=1 ./your_app
   ```

## 配置参数

- `RDMA_INTERCEPT_ENABLE`：启用拦截功能（1=启用，0=禁用）
- `RDMA_INTERCEPT_MAX_QP`：每进程QP数量限制
- `RDMA_INTERCEPT_MAX_GLOBAL_QP`：全局QP数量限制
- `RDMA_INTERCEPT_LOG_LEVEL`：日志级别（0=禁用，1=基本信息，2=详细调试）

## 当前版本的局限性

1. **通信开销**：Unix域套接字通信引入额外延迟
2. **同步开销**：每次RDMA操作都需要查询collector_server
3. **扩展性限制**：高并发场景下collector_server可能成为瓶颈
4. **内存开销**：eBPF maps和collector_server维护额外数据结构

## 后续优化方向

### 1. 共享内存优化
- 使用共享内存替代Unix域套接字
- 实现无锁数据结构减少同步开销
- 采用内存映射文件实现跨进程数据共享

### 2. eBPF程序优化
- 优化kprobe程序减少执行开销
- 实现更智能的事件过滤机制
- 增加批量处理减少系统调用次数

### 3. 架构改进
- 实现多级缓存减少查询延迟
- 采用异步通信模式
- 增加负载均衡机制

## 测试验证

本版本已通过以下测试验证：
- ✅ 功能测试：拦截功能正常工作
- ✅ 性能测试：开销在可接受范围内
- ✅ 资源限制：全局和进程级资源限制有效
- ✅ 稳定性测试：长时间运行无内存泄漏
- ✅ 并发测试：多进程场景下资源统计准确

## 适用场景

- 多租户RDMA环境资源隔离
- 云原生RDMA服务资源管理
- 高性能计算集群资源调度
- RDMA测试和开发环境

## 版本标识

**版本号**：v1.0-unix-socket
**发布日期**：2026-02-05
**架构类型**：Unix域套接字通信架构
**性能等级**：中等（16-26%开销）
**稳定性等级**：高（通过全面测试验证）